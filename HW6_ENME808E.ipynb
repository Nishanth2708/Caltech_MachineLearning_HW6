{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Necessery code and explanation is provided below for the assignment** https://work.caltech.edu/homework/hw6.pdf.\n",
    "\n",
    "**Solution:\n",
    "1. b \n",
    "2. a \n",
    "3. d \n",
    "4. e\n",
    "5. d \n",
    "6. b\n",
    "7. c\n",
    "8. d \n",
    "9. a \n",
    "10. e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1\n",
    "\n",
    "Since, the target function is fixed, we can only change the hypothesis set.As the H' is the subset of the H, we can say that the H' is the smaller model of the main hypothesis and it's obvious that the in sample error would be large Hence, it wouldn't fit the data appropriately and the deterministic noise will **increase**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2\n",
    "\n",
    "**Load Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.loadtxt('http://work.caltech.edu/data/in.dta')\n",
    "test_data = np.loadtxt('http://work.caltech.edu/data/out.dta')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 , x2 = train_data[:,0], train_data[:,1]\n",
    "X1, X2 = test_data[:,0], test_data[:,1]\n",
    "y_train, y_test = train_data[:,2], test_data[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.77947021,  0.15563491, -0.0599077 ,  0.20759636, -0.19598312,\n",
       "         0.58848947,  0.00719859,  0.73883852,  0.70464808,  0.96992666,\n",
       "         0.43543099, -0.84425822,  0.59142471, -0.06909312, -0.95154865,\n",
       "        -0.12988138, -0.49534647, -0.90399413,  0.29235128,  0.64798552,\n",
       "         0.37595574,  0.24588993, -0.45719155, -0.44127876,  0.50744669,\n",
       "        -0.13258381, -0.44749067,  0.81658199, -0.94422408,  0.46265533,\n",
       "         0.88311642,  1.001605  ,  0.60370095, -0.14858757,  0.01165216]),\n",
       " array([ 0.83822138,  0.89537743, -0.71777995,  0.75893338, -0.37548716,\n",
       "        -0.84255381, -0.5483165 , -0.60339369, -0.02042005,  0.6413712 ,\n",
       "         0.74477254,  0.74235423, -0.54602118,  0.03766   , -0.73305502,\n",
       "         0.75676096, -0.56627908,  0.5092215 ,  0.16089015, -0.77933769,\n",
       "         0.07820309,  0.00451467,  0.42390461,  0.70571892,  0.75872586,\n",
       "        -0.58178837,  0.19576364, -0.45449182,  0.88273421,  0.35583605,\n",
       "        -0.19930013,  0.52575476, -0.54553701, -0.21308372,  0.88923931]),\n",
       " array([-0.10600562,  0.17792951,  0.10216153,  0.69407831,  0.02354107,\n",
       "        -0.31972776, -0.18674372, -0.63696719, -0.4744626 , -0.03562765,\n",
       "        -0.14860269, -0.18065154, -0.60241113,  0.69808093,  0.88150888,\n",
       "        -0.92384936, -0.76571338,  0.13559198, -0.15515148,  0.48517476,\n",
       "        -0.6029001 , -0.57285816, -0.63539998,  0.90931654,  0.25210516,\n",
       "        -0.51763422, -0.38587212,  0.82316659,  0.82248638, -0.50366208,\n",
       "         0.53387369, -0.89497008,  0.34287096,  0.70928861, -1.0004317 ,\n",
       "         0.52428439, -0.5603304 ,  0.69752202,  0.49042314, -0.32677396,\n",
       "        -0.00293421, -0.63123891,  0.91388134,  0.2182828 , -0.61618517,\n",
       "        -0.52852914, -0.40652261, -0.22979506, -0.50212113, -0.50808021,\n",
       "        -0.79067809, -0.38251119,  0.82232787,  0.98596447, -0.01404695,\n",
       "        -0.05416513, -1.0724684 , -0.24298545, -0.32448585,  0.24774928,\n",
       "        -0.17221056, -0.41726341, -0.34783792, -0.8518338 , -0.72524439,\n",
       "         0.34532731,  0.74242107, -0.13712343,  0.10591518,  0.33240671,\n",
       "        -0.41754141, -0.40488945, -0.79715796,  0.87592113,  0.54354035,\n",
       "         0.07537975,  0.56404879,  0.81699117, -0.37661111,  0.43602851,\n",
       "         0.39691855, -0.27417659, -0.98918073, -0.51644146,  0.98064342,\n",
       "         0.77755709, -0.25936394, -0.23771804, -0.60395095, -0.28011575,\n",
       "        -0.20373733, -0.16372581,  0.74444529, -0.33226757, -0.02766494,\n",
       "         0.70696684, -0.82301562, -0.98538078, -0.49099732, -0.52247192,\n",
       "        -0.56967959, -0.32370497,  0.91968888,  0.81817666, -0.71264726,\n",
       "         0.53684649,  0.04885657,  0.40044649,  0.72957688, -0.84640286,\n",
       "         0.8307431 ,  0.44642585,  0.63503752,  0.13470715, -0.58076298,\n",
       "        -0.42655272, -0.03091659,  0.48724109, -0.50642751,  0.24941012,\n",
       "        -0.44334283, -0.1045416 , -0.28387756,  1.0121137 ,  0.0618232 ,\n",
       "         0.08713951,  0.87154983, -0.42224414, -0.52995701,  0.59019103,\n",
       "         0.84047379,  0.09185697, -0.08221006,  0.97316077,  0.43572065,\n",
       "         0.52452847,  0.5547334 ,  0.95598815, -0.77089155,  0.88986401,\n",
       "        -0.26199343,  0.64157196, -0.41531211,  0.46745673,  0.47789055,\n",
       "        -0.82623795, -0.94592941,  0.50662354, -0.53612852,  0.41017996,\n",
       "        -0.2616225 ,  0.3191128 ,  0.14756822,  0.62989832, -0.95407922,\n",
       "        -0.24492709,  0.58108886,  0.0410187 ,  0.58396336, -0.48730486,\n",
       "         0.82589939,  0.52174069,  0.83882489, -0.62960544,  0.8524444 ,\n",
       "         0.38323766, -0.05698733,  0.30412897, -0.86141937, -0.86320225,\n",
       "         0.5594634 , -0.41758769,  0.12212703,  0.74665959,  0.20615224,\n",
       "        -0.89098492, -0.68592982, -0.1978071 ,  0.61697403, -0.79341609,\n",
       "        -1.0101707 , -0.86176046, -0.19720509,  0.57561847, -0.61354207,\n",
       "        -0.55181353,  0.1600374 , -0.39716418, -0.57886995,  0.83179248,\n",
       "        -0.29893087, -0.14926315, -0.74495752,  0.15562362,  0.52893831,\n",
       "         0.08282445, -0.93987636,  0.71418089, -0.01112576,  0.54275886,\n",
       "         0.73410595,  0.37896208, -0.16656364, -0.78201587,  0.27177275,\n",
       "        -0.86780139, -0.18120946, -0.04447289,  0.42931482, -0.83797535,\n",
       "         0.69977931,  0.77403264, -0.68813838, -0.42531784,  0.44357173,\n",
       "        -0.00218607,  0.32546528,  0.27126831,  0.20834261, -0.33803178,\n",
       "        -0.02426025,  0.25541631, -0.71661879,  0.13171741,  0.34455847,\n",
       "         0.08243841,  0.53327355, -0.33881875,  0.55124163, -0.41056066,\n",
       "         0.46292501,  0.57568892, -0.63209364, -0.16764181,  0.72036792,\n",
       "         0.75002909,  0.25173   , -0.72459762, -0.7204275 , -0.95384434,\n",
       "         0.71149028,  0.29083625,  0.31974619,  0.23353842, -0.19596922,\n",
       "        -0.43776733, -0.35482986,  0.34704463,  0.83637625, -0.7138508 ]),\n",
       " array([-8.1467034e-02, -3.4595141e-01,  7.1825825e-01,  6.2339743e-01,\n",
       "         7.2743221e-01, -8.3411411e-01,  5.3887798e-01,  1.5268485e-01,\n",
       "         8.5434436e-01, -2.7158819e-01,  1.6176177e-01, -1.2873906e-01,\n",
       "         9.2550746e-01,  7.9474170e-01, -2.0124822e-01,  3.8662506e-01,\n",
       "        -1.1281293e-02,  3.1705057e-02, -3.3141997e-01,  2.9903104e-01,\n",
       "         3.3323420e-01,  8.2835226e-01, -4.7456571e-01, -7.8488922e-01,\n",
       "        -8.9393713e-01,  9.6044364e-01, -3.1786995e-01, -1.2779654e-01,\n",
       "        -8.7684295e-01,  9.8027386e-01,  8.2123374e-01, -2.4011485e-01,\n",
       "         4.7497683e-01,  5.6220681e-01,  6.0457567e-02,  7.3519522e-01,\n",
       "         7.5583819e-01, -6.7198955e-01,  7.8508662e-01,  3.4337193e-01,\n",
       "        -4.1518173e-01,  3.5263395e-01,  5.9305320e-01,  3.9683537e-02,\n",
       "        -8.8657924e-01,  2.8690210e-02,  1.0451480e+00,  7.1425065e-02,\n",
       "         8.3373782e-01,  7.9326965e-01,  1.8780315e-01,  8.2474204e-01,\n",
       "         4.0148690e-01, -3.2916872e-01, -1.5238711e-01,  9.1428474e-01,\n",
       "        -7.2028556e-01, -1.0426515e+00, -2.8317976e-01, -2.5565619e-01,\n",
       "        -8.4939971e-01, -3.9327101e-01, -5.7380853e-01, -7.2266417e-01,\n",
       "        -3.7370735e-01, -2.2271792e-02,  7.4085732e-01, -3.4725580e-01,\n",
       "         6.3378770e-01, -5.6552847e-01,  9.4856269e-01, -6.1346877e-01,\n",
       "         9.0701039e-01,  3.6021089e-01, -1.8109142e-01, -5.1108329e-01,\n",
       "         7.7190965e-01,  5.2558687e-01,  1.0599395e-01,  1.5045963e-01,\n",
       "        -5.4893287e-01,  6.0223846e-01,  1.5764882e-01, -8.2457351e-01,\n",
       "         5.4613841e-01, -8.9346511e-01, -6.4448037e-01, -9.0677112e-01,\n",
       "         8.8173896e-02, -1.5129025e-02,  7.9798618e-01,  4.3600528e-01,\n",
       "         4.1089362e-01, -4.5875081e-01, -3.6080053e-01,  7.5427653e-01,\n",
       "        -3.0287426e-01, -3.8441488e-01,  7.0280304e-01,  3.0000586e-01,\n",
       "         1.0446241e-01,  7.2171478e-01, -3.2574194e-01,  3.4900834e-01,\n",
       "        -4.9117684e-01,  1.0477039e+00,  1.2522709e-01, -1.3359043e-01,\n",
       "        -3.6594220e-01,  8.7034709e-01,  5.8471902e-01,  2.8335762e-01,\n",
       "         8.5261243e-01,  8.4030406e-01, -1.4677565e-02,  4.9157607e-01,\n",
       "         1.0815951e+00, -7.5002965e-01, -9.1039293e-01,  3.8309361e-01,\n",
       "        -7.6390321e-01, -8.8349573e-01, -5.7173283e-01,  3.7187620e-01,\n",
       "        -6.0798489e-01,  3.6089253e-01,  4.1356362e-01,  5.2139273e-01,\n",
       "         6.9943942e-01,  4.4697246e-01, -8.5034269e-01, -2.3185106e-01,\n",
       "        -4.0215845e-01,  6.4157939e-01,  2.7636108e-01, -5.4354500e-01,\n",
       "         3.8297967e-01, -8.0157282e-01,  4.3341968e-01, -5.3114670e-01,\n",
       "         6.6586981e-02, -1.8224977e-01,  7.7832594e-01,  8.1829167e-01,\n",
       "        -2.4897772e-01,  9.4397856e-01, -4.2810919e-01, -8.0927375e-01,\n",
       "         2.7239961e-02,  4.3030737e-01, -5.8031367e-01,  2.1557907e-01,\n",
       "        -8.1499284e-01,  6.9969730e-01,  9.2649187e-01,  1.8247041e-02,\n",
       "         3.1610360e-01, -4.5715509e-01,  6.8267605e-01,  8.6463229e-01,\n",
       "        -4.9791886e-02, -8.8946540e-01, -8.2646301e-01, -1.4889383e-01,\n",
       "        -1.0574672e+00,  6.7882270e-01,  6.0584030e-01, -1.0666795e+00,\n",
       "        -2.2624830e-01,  2.0290163e-01,  1.5353255e-01, -3.2695057e-01,\n",
       "        -2.4896540e-01,  5.5546242e-01,  5.7418978e-01,  4.9858238e-01,\n",
       "        -4.6911031e-01, -1.3615995e-01,  1.2927644e-01,  3.6140632e-01,\n",
       "         7.8496772e-02, -5.7865892e-01,  2.7524501e-01,  9.3704466e-01,\n",
       "        -9.4116391e-01, -2.6804314e-01, -3.4197787e-01,  6.5651695e-01,\n",
       "        -8.7340266e-01, -8.5337474e-02, -4.2892168e-01,  6.5394038e-01,\n",
       "        -7.1933493e-01,  9.2067567e-01,  9.1665351e-01, -6.2795455e-01,\n",
       "        -6.6272682e-01, -2.5977397e-01, -8.4275573e-01,  1.1839569e-01,\n",
       "        -8.9072193e-01, -1.1608314e-01, -4.1054076e-01,  3.7672979e-01,\n",
       "         8.1096707e-01, -6.7610958e-01,  6.8091241e-01,  4.4147201e-04,\n",
       "         8.2890920e-01, -7.6967158e-02, -2.0817600e-01,  5.1157000e-01,\n",
       "         7.9310664e-01, -8.5040532e-01, -2.4214875e-01,  6.9743662e-01,\n",
       "        -1.8578580e-01, -8.5202415e-01, -8.2931143e-01, -8.9404250e-01,\n",
       "        -5.5091178e-01, -2.8836090e-01,  4.1985982e-04, -4.5983099e-01,\n",
       "        -1.2877727e-01, -9.7262595e-01,  2.9512885e-01,  9.1995587e-01,\n",
       "        -8.4634282e-01,  5.1225154e-01, -7.3563881e-01, -5.8952332e-01,\n",
       "        -9.8042189e-01, -5.2920071e-01, -1.0400766e+00, -5.3809352e-01,\n",
       "        -9.6077128e-01,  7.4403736e-02, -5.5720908e-01,  4.7741916e-01,\n",
       "        -9.9011601e-01, -4.4320360e-01, -4.0070957e-01,  6.3672589e-01,\n",
       "        -9.9010507e-01,  1.1653144e-02,  8.1982000e-01, -5.4508368e-01,\n",
       "         3.4383080e-01, -6.4057506e-01]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1, x2,X1,X2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nonlinear input Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = np.zeros([35,8])\n",
    "\n",
    "input_data[:,0] = 1\n",
    "input_data[:,1] = x1\n",
    "input_data[:,2] = x2\n",
    "input_data[:,3] = x1**2\n",
    "input_data[:,4] = x2**2\n",
    "input_data[:,5] = x1*x2\n",
    "input_data[:,6] = abs(x1-x2)\n",
    "input_data[:,7] = abs(x1+x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_test_data = np.zeros([250,8])\n",
    "\n",
    "input_test_data[:,0] = 1\n",
    "input_test_data[:,1] = X1\n",
    "input_test_data[:,2] = X2\n",
    "input_test_data[:,3] = X1**2\n",
    "input_test_data[:,4] = X2**2\n",
    "input_test_data[:,5] = X1*X2\n",
    "input_test_data[:,6] = abs(X1-X2)\n",
    "input_test_data[:,7] = abs(X1+X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.64706706 -0.14505927  0.10154121 -2.03296844 -1.82804373  2.48152945\n",
      "  4.15893861  0.31651714]\n"
     ]
    }
   ],
   "source": [
    "first_weight_term = np.linalg.inv(np.dot(np.transpose(input_data), input_data))\n",
    "second_weight_term = np.dot(first_weight_term, np.transpose(input_data))\n",
    "\n",
    "weight = np.dot(second_weight_term , y_train)\n",
    "print(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = np.sign(np.dot( input_data, weight))\n",
    "predict_test =  np.sign(np.dot( input_test_data, weight))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The E_in for the observed sample is 0.03\n",
      " The E_out for the observed sample is 0.084\n"
     ]
    }
   ],
   "source": [
    "\n",
    "'''second question'''\n",
    "E_in = np.mean(predict != y_train)\n",
    "# E_out = np.mean(predict_test != y_test)\n",
    "\n",
    "print(' The E_in for the observed sample is',round(E_in,2))\n",
    "\n",
    "E_out = np.mean(predict_test != y_test)\n",
    "print(' The E_out for the observed sample is', E_out)\n",
    "# print(E_out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3, 4, 5 and 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#So now this results in the weight vector with regularization, wreg:\n",
    "\n",
    "######################### wreg=(Z⊺Z+λI)−1Z⊺y ############\n",
    "\n",
    "def decay(power):\n",
    "    \n",
    "    dk = regularized_term = 10 ** power\n",
    "\n",
    "    return dk\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "wgt=[]\n",
    "\n",
    "for i in range(-5,5):\n",
    "    first_decay_term = np.linalg.inv(np.dot(np.transpose(input_data), input_data) + decay(i)*np.eye(8))\n",
    "    second_decay_term = np.dot(first_decay_term, np.transpose(input_data))\n",
    "\n",
    "    weight_decay = np.dot(second_decay_term , y_train)\n",
    "    \n",
    "    wgt.append(weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 =[]\n",
    "pt1 =[]\n",
    "for k in wgt:\n",
    "    predict1 = np.sign(np.dot( input_data, k))\n",
    "    predict_test1 =  np.sign(np.dot( input_test_data, k))\n",
    "    \n",
    "    p1.append(predict1)\n",
    "    pt1.append(predict_test1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For K= -5 Ein is 0.02857142857142857.\n",
      "\n",
      "For K= -4 Ein is 0.02857142857142857.\n",
      "\n",
      "For K= -3 Ein is 0.02857142857142857.\n",
      "\n",
      "For K= -2 Ein is 0.02857142857142857.\n",
      "\n",
      "For K= -1 Ein is 0.02857142857142857.\n",
      "\n",
      "For K= 0 Ein is 0.0.\n",
      "\n",
      "For K= 1 Ein is 0.05714285714285714.\n",
      "\n",
      "For K= 2 Ein is 0.2.\n",
      "\n",
      "For K= 3 Ein is 0.37142857142857144.\n",
      "\n",
      "For K= 4 Ein is 0.42857142857142855.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for ind, val in enumerate(p1, -5):\n",
    "    \n",
    "    print('For K= {0}'. format(ind), 'Ein is {0}.\\n' .format(np.mean(val != y_train))) \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For K= -5 Eout is 0.084.\n",
      "\n",
      "For K= -4 Eout is 0.084.\n",
      "\n",
      "For K= -3 Eout is 0.08.\n",
      "\n",
      "For K= -2 Eout is 0.084.\n",
      "\n",
      "For K= -1 Eout is 0.056.\n",
      "\n",
      "For K= 0 Eout is 0.092.\n",
      "\n",
      "For K= 1 Eout is 0.124.\n",
      "\n",
      "For K= 2 Eout is 0.228.\n",
      "\n",
      "For K= 3 Eout is 0.436.\n",
      "\n",
      "For K= 4 Eout is 0.452.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "for ind1 , val1 in enumerate(pt1, -5):\n",
    "    \n",
    "        print('For K= {0}'. format(ind1), 'Eout is {0}.\\n' .format(np.mean(val1 != y_test))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the given hypothesis set\n",
    "\n",
    "[**c**]\n",
    "\n",
    "**H(10,0,3)** =  H2\n",
    "\n",
    "**H(10,0,4)** =  H3\n",
    "\n",
    "**H(10,1,3)** = w0*l0 + w1*l1 + w2* l2 + l3 +l4+.......l10 \n",
    "\n",
    "**H(10,1,4)** = w0*l0 + w1*l1 + w2* l2 + w3*l3 +l4+.......l10 \n",
    "\n",
    "\n",
    "**a**. H2 **union** H3 = H3\n",
    "\n",
    "**b**. invalid hypothesis\n",
    "\n",
    "**c** H2 **intersec** H3 = H2\n",
    "\n",
    "**d** invalid hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The given Neuralnetwork has 3 layers in total, one corresponds to input, second layer is considerd to be hidden layer and the final output is considerd as the output layer.\n",
    "\n",
    "During the computation, Three main steps are followed which are **Forward propagation, Backward propagation and the one that computes error**.\n",
    "\n",
    "There are total of 6 layers in the FP with one unit being biased, 4 units in the second layer (one unit is biased) and the two neurons in the output layer.\n",
    "\n",
    "Based on the number of units described above, FP has a total of ( 6 * 3) + ( 4  * 1) which is of 22 units in the **Forward Propagation**\n",
    "\n",
    "Secondly, during the backward propagation we compute the error difference based on the observed value and the output value,we observe 3 operations here from the output layer to the second layer.\n",
    "\n",
    "In the final Backward propagation we observe, a total of same number of operations as we observed in the Forward propagation because as the weights remain same.\n",
    "\n",
    "**In total, the number of operations are 22+3+22 = 47units** approximated to **45**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of input units are 10, number of hidden units are 36 and one output. In order to obtain minimum possible number of weights, the hidden units should be minimized as much as possible. That is each layer has to be divided into 18 layers that is one biased unit and one neuron unit. So, in total the total number of weights are,\n",
    "\n",
    "**10 * 1 + (18 * 2 ) = 46 weights**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the maximum number of weights, after tweaking with various layers of unique neurons, assuming first layer with 22 inputs and second with 14 units. The max weights possible are, \n",
    "\n",
    "**(10 * 21) + (22 * 13) + 14 = 510**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
